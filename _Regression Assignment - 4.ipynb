{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a7b48-fd2c-47f8-851a-85f120fac131",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Ans:Lasso Regression is a linear regression technique that incorporates a regularization term to prevent overfitting. This regularization term is the L1 norm of the model coefficients.\n",
    "\n",
    "Key Differences from Other Regression Techniques:\n",
    "\n",
    "Ridge Regression: Both Ridge and Lasso use regularization to prevent overfitting. However, Ridge Regression uses the L2 norm, which tends to shrink coefficients towards zero but rarely sets them exactly to zero. Lasso, on the other hand, uses the L1 norm, which can lead to sparse models by setting some coefficients to exactly zero.\n",
    "Ordinary Least Squares (OLS): OLS regression aims to minimize the sum of squared residuals without any regularization. This can lead to overfitting, especially when dealing with noisy data or a large number of features. Lasso, by incorporating regularization, helps to mitigate this issue.\n",
    "Key Benefits of Lasso Regression:\n",
    "\n",
    "Feature Selection: Lasso can automatically select relevant features by setting the coefficients of irrelevant features to zero.\n",
    "Model Interpretability: By reducing the number of features, Lasso can lead to more interpretable models.\n",
    "Prevention of Overfitting: The L1 regularization term helps to prevent overfitting, especially when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e91a17-8f64-4af2-a260-ccf7b01e52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "Ans:The primary advantage of using Lasso Regression for feature selection is its ability to automatically select relevant features and discard irrelevant ones.   \n",
    "\n",
    "By adding an L1 penalty term to the loss function, Lasso regression encourages the model to shrink some coefficients to exactly zero. This effectively eliminates the corresponding features from the model, resulting in a more parsimonious and interpretable model.   \n",
    "\n",
    "Key Benefits:\n",
    "\n",
    "Feature Selection: Identifies the most important features, reducing model complexity and improving interpretability.   \n",
    "Prevention of Overfitting: By eliminating irrelevant features, Lasso regression helps to prevent overfitting, especially in high-dimensional datasets.   \n",
    "Improved Model Performance: A simpler model with fewer features can often lead to better generalization performance and reduced prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed43aff-9272-4e4f-8528-15aa05445df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Ans:Interpreting Coefficients in Lasso Regression\n",
    "\n",
    "Interpreting coefficients in Lasso Regression is similar to interpreting coefficients in Ordinary Least Squares (OLS) regression, with a key difference: Lasso Regression can shrink some coefficients to exactly zero.    interpret the coefficients:\n",
    "\n",
    "Non-zero Coefficients:\n",
    "\n",
    "Sign: The sign of the coefficient indicates the direction of the relationship between the feature and the target variable. A positive coefficient implies a positive relationship, while a negative coefficient implies a negative relationship.   \n",
    "Magnitude: The magnitude of the coefficient represents the strength of the relationship. A larger magnitude indicates a stronger influence of the feature on the target variable.   \n",
    "Zero Coefficients:\n",
    "\n",
    "Features with zero coefficients are considered irrelevant and have been effectively removed from the model.   \n",
    "This feature selection property of Lasso Regression can lead to more parsimonious and interpretable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7a973-51a2-4e04-b012-0a042f478941",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n",
    "Ans:Tuning Parameters in Lasso Regression\n",
    "\n",
    "Lasso Regression primarily relies on a single tuning parameter, alpha (α). This parameter controls the strength of the L1 regularization.\n",
    "\n",
    "Impact of alpha (α):\n",
    "\n",
    "Small alpha:\n",
    "Less regularization, leading to a model that is closer to OLS regression.\n",
    "More features may be included in the model.\n",
    "Large alpha:\n",
    "Strong regularization, leading to a sparser model with fewer features.\n",
    "More features may be excluded from the model.\n",
    "Choosing the Optimal alpha:\n",
    "\n",
    "Selecting the optimal value of alpha is crucial for achieving the best model performance. Common techniques include:\n",
    "\n",
    "Cross-Validation:\n",
    "\n",
    "Split the data into training and validation sets.\n",
    "Train the model with different values of alpha on the training set.\n",
    "Evaluate the performance of each model on the validation set.\n",
    "Select the alpha value that yields the best performance.\n",
    "Grid Search:\n",
    "\n",
    "Create a grid of alpha values to explore.\n",
    "Train the model for each alpha value and evaluate its performance.\n",
    "Select the alpha value that results in the best model.\n",
    "Information Criteria:\n",
    "\n",
    "Use criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to select the optimal alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64561ad-0fb3-47db-a2c2-88e55156935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Ans:Lasso Regression is inherently a linear regression technique. It's designed to model linear relationships between features and the target variable.\n",
    "\n",
    "However, you can extend its applicability to non-linear relationships through feature engineering:\n",
    "\n",
    "Polynomial Features:\n",
    "\n",
    "Create polynomial features by raising the original features to powers (e.g., quadratic, cubic, etc.).\n",
    "This can capture non-linear relationships.\n",
    "Example: If you have a feature x, you can create new features like x^2, x^3, etc.\n",
    "Interaction Terms:\n",
    "\n",
    "Create interaction terms between features to model non-linear interactions.\n",
    "Example: If you have features x and y, you can create a new feature x*y.\n",
    "Non-linear Transformations:\n",
    "\n",
    "Apply non-linear transformations to features, such as log, exponential, or trigonometric functions.\n",
    "This can help capture non-linear patterns in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4aa80-a88d-47be-aeda-b054183aee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6890d-5aad-47a3-a1bb-54c829486d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652dd4c-0ee2-4774-b515-d94ccd47ec07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
