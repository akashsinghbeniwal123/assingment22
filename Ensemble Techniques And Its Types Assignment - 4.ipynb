{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a2399-46d0-4a8b-832f-d855120d0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Random Forest Regressor?\n",
    "Ans:Random Forest Regressor is an ensemble learning method used for regression tasks. It combines multiple decision trees to make more accurate and robust predictions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66b89a-60d8-40d4-a847-a1b26380369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "Ans:Random Forest Regressor employs several techniques to mitigate the risk of overfitting:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating):   \n",
    "\n",
    "Each decision tree in the forest is trained on a different subset of the training data, obtained through random sampling with replacement.   \n",
    "This reduces the variance of individual trees by averaging their predictions.   \n",
    "By exposing each tree to different parts of the data, the risk of overfitting to specific patterns in the training data is minimized.\n",
    "2. Feature Randomization:\n",
    "\n",
    "At each node of a decision tree, only a random subset of features is considered for splitting.   \n",
    "This limits the information available to each tree, preventing it from becoming too specialized and prone to overfitting.\n",
    "3. Ensemble Averaging:\n",
    "\n",
    "The final prediction is the average of the predictions from all the trees in the forest.   \n",
    "This averaging process further reduces the impact of individual tree errors and helps to smooth out the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec2690-9401-43fe-bfbb-2811557cd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "Ans:Random Forest Regressor aggregates the predictions of multiple decision trees through a simple yet effective method: averaging.   \n",
    "\n",
    "Each decision tree in the forest makes an independent prediction for a given input. Once all trees have made their predictions, the final prediction is calculated as the average of these individual predictions.   \n",
    "\n",
    "This averaging process helps to:\n",
    "\n",
    "Reduce Variance: By combining multiple trees, the random forest reduces the variance of the predictions, making them more stable and less sensitive to noise in the training data.   \n",
    "Improve Accuracy: The averaged prediction is often more accurate than the prediction of any individual tree, especially when the trees are diverse and uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83596836-6594-4bcb-be7f-7cf303383bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "Ans:Here are the key hyperparameters of Random Forest Regressor and their impact:\n",
    "\n",
    "n_estimators:\n",
    "\n",
    "Number of trees in the forest.   \n",
    "More trees generally lead to better performance but also increase computational cost.   \n",
    "max_depth:\n",
    "\n",
    "Maximum depth of each tree.   \n",
    "Deeper trees can capture complex patterns but are more prone to overfitting.   \n",
    "Limiting the depth can help prevent overfitting.   \n",
    "min_samples_split:\n",
    "\n",
    "Minimum number of samples required to split an internal node.   \n",
    "A higher value can prevent overfitting by avoiding splitting on noisy or irrelevant features.   \n",
    "min_samples_leaf:\n",
    "\n",
    "Minimum number of samples required to be at a leaf node.   \n",
    "Similar to min_samples_split, it helps prevent overfitting.   \n",
    "max_features:\n",
    "\n",
    "Number of features to consider when looking for the best split at each node.   \n",
    "Fewer features can lead to simpler models and reduce overfitting.\n",
    "bootstrap:\n",
    "\n",
    "Whether bootstrap samples are used when building trees.\n",
    "Bootstrapping helps reduce variance and improve generalization.   \n",
    "criterion:\n",
    "\n",
    "The function to measure the quality of a split.\n",
    "Common options include \"mse\" (mean squared error) for regression and \"gini\" or \"entropy\" for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8c752-1389-41e8-8b5e-ab7c2bb87045",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "Ans:While both Random Forest Regressor and Decision Tree Regressor are popular machine learning algorithms for regression tasks, they have key differences:   \n",
    "\n",
    "Decision Tree Regressor:\n",
    "\n",
    "Single Tree: It consists of a single tree-like model of decisions and their possible consequences.   \n",
    "Overfitting: Prone to overfitting, especially with complex datasets.   \n",
    "Interpretability: Highly interpretable as the decision-making process is easy to visualize.   \n",
    "Less Robust: Sensitive to noise and outliers in the data.\n",
    "Random Forest Regressor:\n",
    "\n",
    "Ensemble of Trees: It combines multiple decision trees to make predictions.   \n",
    "Reduced Overfitting: Less prone to overfitting due to the ensemble nature and random feature selection.   \n",
    "Improved Accuracy: Often provides more accurate predictions than individual decision trees.   \n",
    "Robustness: More robust to noise and outliers.   \n",
    "Less Interpretable: While less interpretable than individual decision trees, feature importance can still be analyzed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcde29e-0aad-4978-b77d-591253e9740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d296d-1fbe-4f69-a160-9135588fcd27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d4e29-4c2e-4357-9a2d-5d762b702e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
